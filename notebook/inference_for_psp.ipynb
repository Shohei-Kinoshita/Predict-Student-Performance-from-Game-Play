{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-06-13T11:58:48.965083Z",
     "iopub.status.busy": "2023-06-13T11:58:48.964267Z",
     "iopub.status.idle": "2023-06-13T11:58:51.322595Z",
     "shell.execute_reply": "2023-06-13T11:58:51.321599Z",
     "shell.execute_reply.started": "2023-06-13T11:58:48.965033Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pickle\n",
    "import polars as pl\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import pyarrow as pa\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T11:58:51.329408Z",
     "iopub.status.busy": "2023-06-13T11:58:51.327211Z",
     "iopub.status.idle": "2023-06-13T11:58:52.623457Z",
     "shell.execute_reply": "2023-06-13T11:58:52.622658Z",
     "shell.execute_reply.started": "2023-06-13T11:58:51.329361Z"
    }
   },
   "outputs": [],
   "source": [
    "dtypes = {\"session_id\": 'int64',\n",
    "          \"index\": np.int16,\n",
    "          \"elapsed_time\": np.int32,\n",
    "          \"event_name\": 'category',\n",
    "          \"name\": 'category',\n",
    "          \"level\": np.int8,\n",
    "          \"page\": np.float16,\n",
    "          \"room_coor_x\": np.float16,\n",
    "          \"room_coor_y\": np.float16,\n",
    "          \"screen_coor_x\": np.float16,\n",
    "          \"screen_coor_y\": np.float16,\n",
    "          \"hover_duration\": np.float32,\n",
    "          \"text\": 'category',\n",
    "          \"fqid\": 'category',\n",
    "          \"room_fqid\": 'category',\n",
    "          \"text_fqid\": 'category',\n",
    "          \"fullscreen\": np.int8,\n",
    "          \"hq\": np.int8,\n",
    "          \"music\": np.int8,\n",
    "          \"level_group\": 'category'\n",
    "          }\n",
    "use_col = ['session_id', 'index', 'elapsed_time', 'event_name', 'name', 'level', 'page',\n",
    "           'room_coor_x', 'room_coor_y', 'hover_duration', 'text', 'fqid', 'room_fqid', 'text_fqid', 'level_group']\n",
    "\n",
    "targets = pd.read_csv('/kaggle/input/predict-student-performance-from-game-play/train_labels.csv')\n",
    "targets['session'] = targets.session_id.apply(lambda x: int(x.split('_')[0]) )\n",
    "targets['q'] = targets.session_id.apply(lambda x: int(x.split('_')[-1][1:]) )\n",
    "\n",
    "feature_df = pd.read_csv('/kaggle/input/feature-for-cat/feature_sort.csv')\n",
    "\n",
    "models = {}\n",
    "\n",
    "list_kol_f = {1: 140, 3: 110, 4: 120, 5: 220, 6: 130, 7: 110, 8: 110, 9: 100, 10: 140, 11: 120, 14: 160, 15: 160, 16: 130, 17: 140}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T11:58:52.629761Z",
     "iopub.status.busy": "2023-06-13T11:58:52.627682Z",
     "iopub.status.idle": "2023-06-13T11:58:52.662580Z",
     "shell.execute_reply": "2023-06-13T11:58:52.661667Z",
     "shell.execute_reply.started": "2023-06-13T11:58:52.629724Z"
    }
   },
   "outputs": [],
   "source": [
    "def delt_time_def(df):\n",
    "    df.sort_values(by=['session_id', 'elapsed_time'], inplace=True)\n",
    "    df['d_time'] = df['elapsed_time'].diff(1)\n",
    "    df['d_time'].fillna(0, inplace=True)\n",
    "    df['delt_time'] = df['d_time'].clip(0, 103000)\n",
    "    df['delt_time_next'] = df['delt_time'].shift(-1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_engineer(train, kol_f):\n",
    "    global kol_col, kol_col_max\n",
    "    kol_col = 9\n",
    "    kol_col_max = 11+kol_f*2\n",
    "    col = [i for i in range(0,kol_col_max)]\n",
    "    new_train = pd.DataFrame(index=train['session_id'].unique(), columns=col, dtype=np.float16)  \n",
    "    new_train[10] = new_train.index # \"session_id\"    \n",
    "\n",
    "    new_train[0] = train.groupby(['session_id'])['d_time'].quantile(q=0.3)\n",
    "    new_train[1] = train.groupby(['session_id'])['d_time'].quantile(q=0.8)\n",
    "    new_train[2] = train.groupby(['session_id'])['d_time'].quantile(q=0.5)\n",
    "    new_train[3] = train.groupby(['session_id'])['d_time'].quantile(q=0.65)\n",
    "    new_train[4] = train.groupby(['session_id'])['hover_duration'].agg('mean')\n",
    "    new_train[5] = train.groupby(['session_id'])['hover_duration'].agg('std')    \n",
    "    new_train[6] = new_train[10].apply(lambda x: int(str(x)[:2])).astype(np.uint8) # \"year\"\n",
    "    new_train[7] = new_train[10].apply(lambda x: int(str(x)[2:4])+1).astype(np.uint8) # \"month\"\n",
    "    new_train[8] = new_train[10].apply(lambda x: int(str(x)[4:6])).astype(np.uint8) # \"day\"\n",
    "    new_train[9] = new_train[10].apply(lambda x: int(str(x)[6:8])).astype(np.uint8) + new_train[10].apply(lambda x: int(str(x)[8:10])).astype(np.uint8)/60\n",
    "    new_train[10] = 0\n",
    "    new_train = new_train.fillna(-1)\n",
    "    \n",
    "    return new_train\n",
    "\n",
    "\n",
    "def feature_next_t(row_f, new_train, train, gran_1, gran_2, i):\n",
    "    global kol_col\n",
    "    kol_col +=1\n",
    "    col1 = row_f['col1']\n",
    "    val1 = row_f['val1']\n",
    "    maska = (train[col1] == val1)\n",
    "    if row_f['kol_col'] == 1:       \n",
    "        new_train[kol_col] = train[maska].groupby(['session_id'])['delt_time_next'].sum()\n",
    "        if gran_1:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska].groupby(['session_id'])['delt_time'].mean()\n",
    "        if gran_2:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska].groupby(['session_id'])['index'].count()          \n",
    "    elif row_f['kol_col'] == 2: \n",
    "        col2 = row_f['col2']\n",
    "        val2 = row_f['val2']\n",
    "        maska = maska & (train[col2] == val2)        \n",
    "        new_train[kol_col] = train[maska].groupby(['session_id'])['delt_time_next'].sum()\n",
    "        if gran_1:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska].groupby(['session_id'])['delt_time'].mean()\n",
    "        if gran_2:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska].groupby(['session_id'])['index'].count()\n",
    "    return new_train\n",
    "\n",
    "\n",
    "def feature_next_t_otvet(row_f, new_train, train, gran_1, gran_2, i):\n",
    "    global kol_col\n",
    "    kol_col +=1\n",
    "    col1 = row_f['col1']\n",
    "    val1 = row_f['val1']\n",
    "    maska = (train[col1] == val1)\n",
    "    if row_f['kol_col'] == 1:      \n",
    "        new_train[kol_col] = train[maska]['delt_time_next'].sum()\n",
    "        if gran_1:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska]['delt_time'].mean()\n",
    "        if gran_2:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska]['index'].count()          \n",
    "    elif row_f['kol_col'] == 2: \n",
    "        col2 = row_f['col2']\n",
    "        val2 = row_f['val2']\n",
    "        maska = maska & (train[col2] == val2)        \n",
    "        new_train[kol_col] = train[maska]['delt_time_next'].sum()\n",
    "        if gran_1:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska]['delt_time'].mean()\n",
    "        if gran_2:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska]['index'].count()\n",
    "    return new_train\n",
    "\n",
    "\n",
    "def experiment_feature_next_t_otvet(row_f, new_train, train, gran_1, gran_2, i):\n",
    "    global kol_col\n",
    "    kol_col +=1\n",
    "    if row_f['kol_col'] == 1: \n",
    "        maska = train[row_f['col1']] == row_f['val1']\n",
    "        new_train[kol_col] = train[maska]['delt_time_next'].sum()\n",
    "        if gran_1:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska]['delt_time'].mean()\n",
    "        if gran_2:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska]['index'].count()          \n",
    "    elif row_f['kol_col'] == 2: \n",
    "        col2 = row_f['col2']\n",
    "        val2 = row_f['val2']\n",
    "        maska = (train[col1] == val1) & (train[col2] == val2)        \n",
    "        new_train[kol_col] = train[maska]['delt_time_next'].sum()\n",
    "        if gran_1:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska]['delt_time'].mean()\n",
    "        if gran_2:\n",
    "            kol_col +=1\n",
    "            new_train[kol_col] = train[maska]['index'].count()\n",
    "    return new_train\n",
    "\n",
    "\n",
    "def feature_quest_otvet(new_train, train, quest, kol_f):\n",
    "    global kol_col\n",
    "    kol_col = 9\n",
    "    g1 = 0.7 \n",
    "    g2 = 0.3 \n",
    "\n",
    "    feature_q = feature_df[feature_df['quest'] == quest].copy()\n",
    "    feature_q.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    gran1 = round(kol_f * g1)\n",
    "    gran2 = round(kol_f * g2)    \n",
    "    for i in range(0, kol_f):         \n",
    "        row_f = feature_q.loc[i]\n",
    "        new_train = feature_next_t_otvet(row_f, new_train, train, i < gran1, i <  gran2, i) \n",
    "    col = [i for i in range(0,kol_col+1)]\n",
    "    return new_train[col]\n",
    "\n",
    "\n",
    "def feature_engineer_new(new_train, train, feature_q, kol_f):\n",
    "    g1 = 0.7 \n",
    "    g2 = 0.3 \n",
    "    gran1 = round(kol_f * g1)\n",
    "    gran2 = round(kol_f * g2)    \n",
    "    for i in range(0, kol_f): \n",
    "        row_f = feature_q.loc[i]       \n",
    "        new_train = feature_next_t(row_f, new_train, train, i < gran1, i <  gran2, i)         \n",
    "    return new_train\n",
    "\n",
    "\n",
    "def feature_quest(new_train, train, quest, kol_f):\n",
    "    global kol_col\n",
    "    kol_col = 9\n",
    "    feature_q = feature_df[feature_df['quest'] == quest].copy()\n",
    "    feature_q.reset_index(drop=True, inplace=True)\n",
    "    new_train = feature_engineer_new(new_train, train, feature_q, kol_f)\n",
    "    col = [i for i in range(0,kol_col+1)]\n",
    "    return new_train[col]\n",
    "\n",
    "\n",
    "def create_model(old_train, quests, models, list_kol_f):\n",
    "    \n",
    "    kol_quest = len(quests)\n",
    "    # ITERATE THRU QUESTIONS\n",
    "    for q in quests:\n",
    "        print('### quest ', q, end='')\n",
    "        new_train = feature_engineer(old_train, list_kol_f[q])\n",
    "        train_x = feature_quest(new_train, old_train, q, list_kol_f[q])\n",
    "        print (' ---- ', 'train_q.shape = ', train_x.shape)\n",
    "           \n",
    "        # TRAIN DATA\n",
    "        train_users = train_x.index.values\n",
    "        train_y = targets.loc[targets.q==q].set_index('session').loc[train_users]\n",
    "\n",
    "        # TRAIN MODEL \n",
    "\n",
    "        model = CatBoostClassifier(\n",
    "            n_estimators = 300,\n",
    "            learning_rate= 0.045,\n",
    "            depth = 6\n",
    "        )\n",
    "        \n",
    "        model.fit(train_x.astype('float32'), train_y['correct'], verbose=False)\n",
    "\n",
    "        # SAVE MODEL, PREDICT VALID OOF\n",
    "        models[f'{q}'] = model\n",
    "    print('***')\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T11:59:10.964422Z",
     "iopub.status.busy": "2023-06-13T11:59:10.963968Z",
     "iopub.status.idle": "2023-06-13T12:00:33.210137Z",
     "shell.execute_reply": "2023-06-13T12:00:33.207926Z",
     "shell.execute_reply.started": "2023-06-13T11:59:10.964388Z"
    }
   },
   "outputs": [],
   "source": [
    "df0_4 = pd.read_csv('/kaggle/input/feature-for-cat/train_0_4t.csv', dtype=dtypes) \n",
    "kol_lvl = (df0_4 .groupby(['session_id'])['level'].agg('nunique') < 5)\n",
    "list_session = kol_lvl[kol_lvl].index\n",
    "df0_4  = df0_4 [~df0_4 ['session_id'].isin(list_session)]\n",
    "df0_4 = delt_time_def(df0_4)\n",
    "\n",
    "quests_0_4 = [1, 3] \n",
    "\n",
    "models = create_model(df0_4, quests_0_4, models, list_kol_f)\n",
    "del df0_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T12:00:33.212514Z",
     "iopub.status.busy": "2023-06-13T12:00:33.212172Z",
     "iopub.status.idle": "2023-06-13T12:07:51.212805Z",
     "shell.execute_reply": "2023-06-13T12:07:51.211892Z",
     "shell.execute_reply.started": "2023-06-13T12:00:33.212485Z"
    }
   },
   "outputs": [],
   "source": [
    "df5_12 = pd.read_csv('/kaggle/input/feature-for-cat/train_5_12t.csv', dtype=dtypes)\n",
    "kol_lvl = (df5_12.groupby(['session_id'])['level'].agg('nunique') < 8)\n",
    "list_session = kol_lvl[kol_lvl].index\n",
    "df5_12 = df5_12[~df5_12['session_id'].isin(list_session)]\n",
    "df5_12 = delt_time_def(df5_12)\n",
    "quests_5_12 = [4, 5, 6, 7, 8, 9, 10, 11] \n",
    "\n",
    "models = create_model(df5_12, quests_5_12, models, list_kol_f)\n",
    "del df5_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T12:07:51.214472Z",
     "iopub.status.busy": "2023-06-13T12:07:51.214129Z",
     "iopub.status.idle": "2023-06-13T12:12:52.587912Z",
     "shell.execute_reply": "2023-06-13T12:12:52.586682Z",
     "shell.execute_reply.started": "2023-06-13T12:07:51.214443Z"
    }
   },
   "outputs": [],
   "source": [
    "df13_22 = pd.read_csv('/kaggle/input/feature-for-cat/train_13_22t.csv', dtype=dtypes) \n",
    "kol_lvl = (df13_22 .groupby(['session_id'])['level'].agg('nunique') < 10)\n",
    "list_session = kol_lvl[kol_lvl].index\n",
    "df13_22  = df13_22 [~df13_22 ['session_id'].isin(list_session)]\n",
    "df13_22 = delt_time_def(df13_22)\n",
    "\n",
    "quests_13_22 = [14, 15, 16, 17] \n",
    "\n",
    "models = create_model(df13_22, quests_13_22, models, list_kol_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGboost Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T12:12:52.592674Z",
     "iopub.status.busy": "2023-06-13T12:12:52.591361Z",
     "iopub.status.idle": "2023-06-13T12:12:52.612618Z",
     "shell.execute_reply": "2023-06-13T12:12:52.611685Z",
     "shell.execute_reply.started": "2023-06-13T12:12:52.592636Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "LIST_CATS = ['event_name', 'name', 'fqid', 'room_fqid', 'text_fqid']\n",
    "LIST_NUMS = ['page', 'room_coor_x', 'room_coor_y', 'screen_coor_x', 'screen_coor_y', 'hover_duration', 'elapsed_time_diff']\n",
    "\n",
    "LIST_NAME_FEATURE = ['basic', 'undefined', 'close', 'open', 'prev', 'next']\n",
    "LIST_EVENT_NAME_FEATURE = ['cutscene_click', 'person_click', 'navigate_click', 'observation_click', 'notification_click', 'object_click', 'object_hover', 'map_hover', 'map_click', 'checkpoint', 'notebook_click']\n",
    "\n",
    "LIST_FQID = [\n",
    "     'worker',\n",
    "     'archivist',\n",
    "     'gramps',\n",
    "     'wells',\n",
    "     'toentry',\n",
    "     'confrontation',\n",
    "     'crane_ranger',\n",
    "     'groupconvo',\n",
    "     'flag_girl',\n",
    "     'tomap',\n",
    "     'tostacks',\n",
    "     'tobasement',\n",
    "     'archivist_glasses',\n",
    "     'boss',\n",
    "     'journals',\n",
    "     'seescratches',\n",
    "     'groupconvo_flag',\n",
    "     'cs',\n",
    "     'teddy',\n",
    "     'expert',\n",
    "     'businesscards',\n",
    "     'ch3start',\n",
    "     'tunic.historicalsociety',\n",
    "     'tofrontdesk',\n",
    "     'savedteddy',\n",
    "     'plaque',\n",
    "     'glasses',\n",
    "     'tunic.drycleaner',\n",
    "     'reader_flag',\n",
    "     'tunic.library',\n",
    "     'tracks',\n",
    "     'tunic.capitol_2',\n",
    "     'trigger_scarf',\n",
    "     'reader',\n",
    "     'directory',\n",
    "     'tunic.capitol_1',\n",
    "     'journals.pic_0.next',\n",
    "     'unlockdoor',\n",
    "     'tunic',\n",
    "     'what_happened',\n",
    "     'tunic.kohlcenter',\n",
    "     'tunic.humanecology',\n",
    "     'colorbook',\n",
    "     'logbook',\n",
    "     'businesscards.card_0.next',\n",
    "     'journals.hub.topics',\n",
    "     'logbook.page.bingo',\n",
    "     'journals.pic_1.next',\n",
    "     'journals_flag',\n",
    "     'reader.paper0.next',\n",
    "     'tracks.hub.deer',\n",
    "     'reader_flag.paper0.next',\n",
    "     'trigger_coffee',\n",
    "     'wellsbadge',\n",
    "     'journals.pic_2.next',\n",
    "     'tomicrofiche',\n",
    "     'journals_flag.pic_0.bingo',\n",
    "     'plaque.face.date',\n",
    "     'notebook',\n",
    "     'tocloset_dirty',\n",
    "     'businesscards.card_bingo.bingo',\n",
    "     'businesscards.card_1.next',\n",
    "     'tunic.wildlife',\n",
    "     'tunic.hub.slip',\n",
    "     'tocage',\n",
    "     'journals.pic_2.bingo',\n",
    "     'tocollectionflag',\n",
    "     'tocollection',\n",
    "     'chap4_finale_c',\n",
    "     'chap2_finale_c',\n",
    "     'lockeddoor',\n",
    "     'journals_flag.hub.topics',\n",
    "     'tunic.capitol_0',\n",
    "     'reader_flag.paper2.bingo',\n",
    "     'photo',\n",
    "     'tunic.flaghouse',\n",
    "     'reader.paper1.next',\n",
    "     'directory.closeup.archivist',\n",
    "     'intro',\n",
    "     'businesscards.card_bingo.next',\n",
    "     'reader.paper2.bingo',\n",
    "     'retirement_letter',\n",
    "     'remove_cup',\n",
    "     'journals_flag.pic_0.next',\n",
    "     'magnify',\n",
    "     'coffee',\n",
    "     'key',\n",
    "     'togrampa',\n",
    "     'reader_flag.paper1.next',\n",
    "     'janitor',\n",
    "     'tohallway',\n",
    "     'chap1_finale',\n",
    "     'report',\n",
    "     'outtolunch',\n",
    "     'journals_flag.hub.topics_old',\n",
    "     'journals_flag.pic_1.next',\n",
    "     'reader.paper2.next',\n",
    "     'chap1_finale_c',\n",
    "     'reader_flag.paper2.next',\n",
    "     'door_block_talk',\n",
    "     'journals_flag.pic_1.bingo',\n",
    "     'journals_flag.pic_2.next',\n",
    "     'journals_flag.pic_2.bingo',\n",
    "     'block_magnify',\n",
    "     'reader.paper0.prev',\n",
    "     'block',\n",
    "     'reader_flag.paper0.prev',\n",
    "     'block_0',\n",
    "     'door_block_clean',\n",
    "     'reader.paper2.prev',\n",
    "     'reader.paper1.prev',\n",
    "     'doorblock',\n",
    "     'tocloset',\n",
    "     'reader_flag.paper2.prev',\n",
    "     'reader_flag.paper1.prev',\n",
    "     'block_tomap2',\n",
    "     'journals_flag.pic_0_old.next',\n",
    "     'journals_flag.pic_1_old.next',\n",
    "     'block_tocollection',\n",
    "     'block_nelson',\n",
    "     'journals_flag.pic_2_old.next',\n",
    "     'block_tomap1',\n",
    "     'block_badge',\n",
    "     'need_glasses',\n",
    "     'block_badge_2',\n",
    "     'fox',\n",
    "     'block_1'\n",
    "]\n",
    "\n",
    "LIST_TEXT = [\n",
    "     'tunic.historicalsociety.cage.confrontation',\n",
    "     'tunic.wildlife.center.crane_ranger.crane',\n",
    "     'tunic.historicalsociety.frontdesk.archivist.newspaper',\n",
    "     'tunic.historicalsociety.entry.groupconvo',\n",
    "     'tunic.wildlife.center.wells.nodeer',\n",
    "     'tunic.historicalsociety.frontdesk.archivist.have_glass',\n",
    "     'tunic.drycleaner.frontdesk.worker.hub',\n",
    "     'tunic.historicalsociety.closet_dirty.gramps.news',\n",
    "     'tunic.humanecology.frontdesk.worker.intro',\n",
    "     'tunic.historicalsociety.frontdesk.archivist_glasses.confrontation',\n",
    "     'tunic.historicalsociety.basement.seescratches',\n",
    "     'tunic.historicalsociety.collection.cs',\n",
    "     'tunic.flaghouse.entry.flag_girl.hello',\n",
    "     'tunic.historicalsociety.collection.gramps.found',\n",
    "     'tunic.historicalsociety.basement.ch3start',\n",
    "     'tunic.historicalsociety.entry.groupconvo_flag',\n",
    "     'tunic.library.frontdesk.worker.hello',\n",
    "     'tunic.library.frontdesk.worker.wells',\n",
    "     'tunic.historicalsociety.collection_flag.gramps.flag',\n",
    "     'tunic.historicalsociety.basement.savedteddy',\n",
    "     'tunic.library.frontdesk.worker.nelson',\n",
    "     'tunic.wildlife.center.expert.removed_cup',\n",
    "     'tunic.library.frontdesk.worker.flag',\n",
    "     'tunic.historicalsociety.frontdesk.archivist.hello',\n",
    "     'tunic.historicalsociety.closet.gramps.intro_0_cs_0',\n",
    "     'tunic.historicalsociety.entry.boss.flag',\n",
    "     'tunic.flaghouse.entry.flag_girl.symbol',\n",
    "     'tunic.historicalsociety.closet_dirty.trigger_scarf',\n",
    "     'tunic.drycleaner.frontdesk.worker.done',\n",
    "     'tunic.historicalsociety.closet_dirty.what_happened',\n",
    "     'tunic.wildlife.center.wells.animals',\n",
    "     'tunic.historicalsociety.closet.teddy.intro_0_cs_0',\n",
    "     'tunic.historicalsociety.cage.glasses.afterteddy',\n",
    "     'tunic.historicalsociety.cage.teddy.trapped',\n",
    "     'tunic.historicalsociety.cage.unlockdoor',\n",
    "     'tunic.historicalsociety.stacks.journals.pic_2.bingo',\n",
    "     'tunic.historicalsociety.entry.wells.flag',\n",
    "     'tunic.humanecology.frontdesk.worker.badger',\n",
    "     'tunic.historicalsociety.stacks.journals_flag.pic_0.bingo',\n",
    "     'tunic.historicalsociety.closet.intro',\n",
    "     'tunic.historicalsociety.closet.retirement_letter.hub',\n",
    "     'tunic.historicalsociety.entry.directory.closeup.archivist',\n",
    "     'tunic.historicalsociety.collection.tunic.slip',\n",
    "     'tunic.kohlcenter.halloffame.plaque.face.date',\n",
    "     'tunic.historicalsociety.closet_dirty.trigger_coffee',\n",
    "     'tunic.drycleaner.frontdesk.logbook.page.bingo',\n",
    "     'tunic.library.microfiche.reader.paper2.bingo',\n",
    "     'tunic.kohlcenter.halloffame.togrampa',\n",
    "     'tunic.capitol_2.hall.boss.haveyougotit',\n",
    "     'tunic.wildlife.center.wells.nodeer_recap',\n",
    "     'tunic.historicalsociety.cage.glasses.beforeteddy',\n",
    "     'tunic.historicalsociety.closet_dirty.gramps.helpclean',\n",
    "     'tunic.wildlife.center.expert.recap',\n",
    "     'tunic.historicalsociety.frontdesk.archivist.have_glass_recap',\n",
    "     'tunic.historicalsociety.stacks.journals_flag.pic_1.bingo',\n",
    "     'tunic.historicalsociety.cage.lockeddoor',\n",
    "     'tunic.historicalsociety.stacks.journals_flag.pic_2.bingo',\n",
    "     'tunic.historicalsociety.collection.gramps.lost',\n",
    "     'tunic.historicalsociety.closet.notebook',\n",
    "     'tunic.historicalsociety.frontdesk.magnify',\n",
    "     'tunic.humanecology.frontdesk.businesscards.card_bingo.bingo',\n",
    "     'tunic.wildlife.center.remove_cup',\n",
    "     'tunic.library.frontdesk.wellsbadge.hub',\n",
    "     'tunic.wildlife.center.tracks.hub.deer',\n",
    "     'tunic.historicalsociety.frontdesk.key',\n",
    "     'tunic.library.microfiche.reader_flag.paper2.bingo',\n",
    "     'tunic.flaghouse.entry.colorbook',\n",
    "     'tunic.wildlife.center.coffee',\n",
    "     'tunic.capitol_1.hall.boss.haveyougotit',\n",
    "     'tunic.historicalsociety.basement.janitor',\n",
    "     'tunic.historicalsociety.collection_flag.gramps.recap',\n",
    "     'tunic.wildlife.center.wells.animals2',\n",
    "     'tunic.flaghouse.entry.flag_girl.symbol_recap',\n",
    "     'tunic.historicalsociety.closet_dirty.photo',\n",
    "     'tunic.historicalsociety.stacks.outtolunch',\n",
    "     'tunic.library.frontdesk.worker.wells_recap',\n",
    "     'tunic.historicalsociety.frontdesk.archivist_glasses.confrontation_recap',\n",
    "     'tunic.capitol_0.hall.boss.talktogramps',\n",
    "     'tunic.historicalsociety.closet.photo',\n",
    "     'tunic.historicalsociety.collection.tunic',\n",
    "     'tunic.historicalsociety.closet.teddy.intro_0_cs_5',\n",
    "     'tunic.historicalsociety.closet_dirty.gramps.archivist',\n",
    "     'tunic.historicalsociety.closet_dirty.door_block_talk',\n",
    "     'tunic.historicalsociety.entry.boss.flag_recap',\n",
    "     'tunic.historicalsociety.frontdesk.archivist.need_glass_0',\n",
    "     'tunic.historicalsociety.entry.wells.talktogramps',\n",
    "     'tunic.historicalsociety.frontdesk.block_magnify',\n",
    "     'tunic.historicalsociety.frontdesk.archivist.foundtheodora',\n",
    "     'tunic.historicalsociety.closet_dirty.gramps.nothing',\n",
    "     'tunic.historicalsociety.closet_dirty.door_block_clean',\n",
    "     'tunic.capitol_1.hall.boss.writeitup',\n",
    "     'tunic.library.frontdesk.worker.nelson_recap',\n",
    "     'tunic.library.frontdesk.worker.hello_short',\n",
    "     'tunic.historicalsociety.stacks.block',\n",
    "     'tunic.historicalsociety.frontdesk.archivist.need_glass_1',\n",
    "     'tunic.historicalsociety.entry.boss.talktogramps',\n",
    "     'tunic.historicalsociety.frontdesk.archivist.newspaper_recap',\n",
    "     'tunic.historicalsociety.entry.wells.flag_recap',\n",
    "     'tunic.drycleaner.frontdesk.worker.done2',\n",
    "     'tunic.library.frontdesk.worker.flag_recap',\n",
    "     'tunic.humanecology.frontdesk.block_0',\n",
    "     'tunic.library.frontdesk.worker.preflag',\n",
    "     'tunic.historicalsociety.basement.gramps.seeyalater',\n",
    "     'tunic.flaghouse.entry.flag_girl.hello_recap',\n",
    "     'tunic.historicalsociety.closet.doorblock',\n",
    "     'tunic.drycleaner.frontdesk.worker.takealook',\n",
    "     'tunic.historicalsociety.basement.gramps.whatdo',\n",
    "     'tunic.library.frontdesk.worker.droppedbadge',\n",
    "     'tunic.historicalsociety.entry.block_tomap2',\n",
    "     'tunic.library.frontdesk.block_nelson',\n",
    "     'tunic.library.microfiche.block_0',\n",
    "     'tunic.historicalsociety.entry.block_tocollection',\n",
    "     'tunic.historicalsociety.entry.block_tomap1',\n",
    "     'tunic.historicalsociety.collection.gramps.look_0',\n",
    "     'tunic.library.frontdesk.block_badge',\n",
    "     'tunic.historicalsociety.cage.need_glasses',\n",
    "     'tunic.library.frontdesk.block_badge_2',\n",
    "     'tunic.kohlcenter.halloffame.block_0',\n",
    "     'tunic.capitol_0.hall.chap1_finale_c',\n",
    "     'tunic.capitol_1.hall.chap2_finale_c',\n",
    "     'tunic.capitol_2.hall.chap4_finale_c',\n",
    "     'tunic.wildlife.center.fox.concern',\n",
    "     'tunic.drycleaner.frontdesk.block_0',\n",
    "     'tunic.historicalsociety.entry.gramps.hub',\n",
    "     'tunic.humanecology.frontdesk.block_1',\n",
    "     'tunic.drycleaner.frontdesk.block_1'\n",
    "]\n",
    "\n",
    "LIST_ROOM = [\n",
    "     'tunic.historicalsociety.entry',\n",
    "     'tunic.wildlife.center',\n",
    "     'tunic.historicalsociety.cage',\n",
    "     'tunic.library.frontdesk',\n",
    "     'tunic.historicalsociety.frontdesk',\n",
    "     'tunic.historicalsociety.stacks',\n",
    "     'tunic.historicalsociety.closet_dirty',\n",
    "     'tunic.humanecology.frontdesk',\n",
    "     'tunic.historicalsociety.basement',\n",
    "     'tunic.kohlcenter.halloffame',\n",
    "     'tunic.library.microfiche',\n",
    "     'tunic.drycleaner.frontdesk',\n",
    "     'tunic.historicalsociety.collection',\n",
    "     'tunic.historicalsociety.closet',\n",
    "     'tunic.flaghouse.entry',\n",
    "     'tunic.historicalsociety.collection_flag',\n",
    "     'tunic.capitol_1.hall',\n",
    "     'tunic.capitol_0.hall',\n",
    "     'tunic.capitol_2.hall'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T12:12:52.614434Z",
     "iopub.status.busy": "2023-06-13T12:12:52.613902Z",
     "iopub.status.idle": "2023-06-13T12:12:52.634426Z",
     "shell.execute_reply": "2023-06-13T12:12:52.633388Z",
     "shell.execute_reply.started": "2023-06-13T12:12:52.614394Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_MODEL_PRE_RAW = '/kaggle/input/blend-3ways-feature/xgb_models/pre_raw/XGB_question{}.xgb'\n",
    "PATH_FEATURE_PRE_RAW = '/kaggle/input/blend-3ways-feature/pre_raw_features.json'\n",
    "\n",
    "DIALOGS = ['that', 'this', 'it', 'you','find','found','Found','notebook','Wells','wells','help','need', 'Oh','Ooh','Jo', 'flag', 'can','and','is','the','to']\n",
    "\n",
    "LEVELS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
    "\n",
    "IS_USE_EXTRA = True\n",
    "\n",
    "threshold = 0.625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T12:12:52.636327Z",
     "iopub.status.busy": "2023-06-13T12:12:52.635786Z",
     "iopub.status.idle": "2023-06-13T12:12:52.665636Z",
     "shell.execute_reply": "2023-06-13T12:12:52.664529Z",
     "shell.execute_reply.started": "2023-06-13T12:12:52.636287Z"
    }
   },
   "outputs": [],
   "source": [
    "DICT_FEATURE_LIST = {}\n",
    "with open(PATH_FEATURE_PRE_RAW, mode='r') as fp:\n",
    "    dict_use_feature = json.load(fp)\n",
    "\n",
    "DICT_FEATURE_LIST['pre_raw'] = dict_use_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T12:12:52.667821Z",
     "iopub.status.busy": "2023-06-13T12:12:52.667477Z",
     "iopub.status.idle": "2023-06-13T12:12:52.745206Z",
     "shell.execute_reply": "2023-06-13T12:12:52.743998Z",
     "shell.execute_reply.started": "2023-06-13T12:12:52.667794Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing_pl(x):\n",
    "    \"\"\"preprocessiong raw data before feature enginnering.\"\"\"\n",
    "    columns = [\n",
    "        pl.col(\"page\").cast(pl.Float32),\n",
    "        (\n",
    "            (pl.col(\"elapsed_time\").diff(1))\n",
    "             .fill_null(0)\n",
    "             .clip(0, 1e9)\n",
    "             .over([\"session_id\", \"level_group\"])\n",
    "             .alias(\"elapsed_time_diff\")\n",
    "        ),\n",
    "        (\n",
    "            (pl.col(\"screen_coor_x\").diff(1))\n",
    "             .abs()\n",
    "             .over([\"session_id\", \"level_group\"])\n",
    "            .alias(\"location_x_diff\")\n",
    "        ),\n",
    "        (\n",
    "            (pl.col(\"screen_coor_y\").diff(1))\n",
    "             .abs()\n",
    "             .over([\"session_id\", \"level_group\"])\n",
    "            .alias(\"location_y_diff\")\n",
    "        ),\n",
    "        pl.col(\"fqid\").fill_null(\"fqid_None\"),\n",
    "        pl.col(\"text_fqid\").fill_null(\"text_fqid_None\")\n",
    "    ]\n",
    "\n",
    "    x = (\n",
    "          x.drop([\"fullscreen\", \"hq\", \"music\"])\n",
    "          .with_columns(columns))\n",
    "    return x\n",
    "\n",
    "\n",
    "def feature_engineer_pl(x, grp, use_extra, feature_suffix):\n",
    "    \"\"\"create aggregate features for polars.\"\"\"\n",
    "    def except_handling(s):\n",
    "        try:\n",
    "            return s.max() - s.min()\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    aggs = [\n",
    "        pl.col(\"index\").count().alias(f\"session_number_{feature_suffix}\"),\n",
    "        pl.col(\"text_fqid\").filter(pl.col(\"text_fqid\") == 'text_fqid_None').count().alias(f'null_count'),\n",
    "        pl.col('fqid').filter(pl.col('fqid').str.starts_with('to')).count().alias('page_change_count'),\n",
    "        pl.col('index').filter(pl.col('text').str.contains('?', literal=True)).count().alias('question_count'),\n",
    "        pl.col('index').filter((pl.col(\"event_name\") == 'observation_click') & (pl.col('text_fqid').str.contains('block'))).count().alias('block_count'),\n",
    "        pl.col('index').filter((pl.col(\"event_name\") == 'person_click') & (pl.col('text_fqid').str.contains('recap'))).count().alias('recap_count'),\n",
    "        pl.col('index').filter((pl.col(\"event_name\") == 'person_click') & (pl.col('text_fqid').str.contains('lost'))).count().alias('lost_count'),\n",
    "\n",
    "        *[pl.col(c).drop_nulls().n_unique().alias(f\"{c}_unique_{feature_suffix}\") for c in LIST_CATS],\n",
    "        *[pl.col('index').filter(pl.col('text').str.contains(c, literal=True)).count().alias(f'word_{c}') for c in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c, literal=True))).mean().alias(f'word_mean_{c}') for c in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c, literal=True))).std().alias(f'word_std_{c}') for c in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c, literal=True))).max().alias(f'word_max_{c}') for c in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c, literal=True))).sum().alias(f'word_sum_{c}') for c in DIALOGS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter((pl.col('text').str.contains(c, literal=True))).median().alias(f'word_median_{c}') for c in DIALOGS],\n",
    "\n",
    "        *[pl.col(c).mean().alias(f\"{c}_mean_{feature_suffix}\") for c in LIST_NUMS],\n",
    "        *[pl.col(c).median().alias(f\"{c}_median_{feature_suffix}\") for c in LIST_NUMS],\n",
    "        *[pl.col(c).std().alias(f\"{c}_std_{feature_suffix}\") for c in LIST_NUMS],\n",
    "        *[pl.col(c).min().alias(f\"{c}_min_{feature_suffix}\") for c in LIST_NUMS],\n",
    "        *[pl.col(c).max().alias(f\"{c}_max_{feature_suffix}\") for c in LIST_NUMS],\n",
    "        *[pl.col(c).sum().alias(f\"{c}_sum_{feature_suffix}\") for c in LIST_NUMS],\n",
    "\n",
    "        *[pl.col(\"event_name\").filter(pl.col(\"event_name\") == c).count().alias(f\"{c}_event_name_counts{feature_suffix}\")for c in LIST_EVENT_NAME_FEATURE],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\")==c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in LIST_EVENT_NAME_FEATURE],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\")==c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for c in LIST_EVENT_NAME_FEATURE],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\")==c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in LIST_EVENT_NAME_FEATURE],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"event_name\")==c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in LIST_EVENT_NAME_FEATURE],\n",
    "\n",
    "        *[pl.col(\"name\").filter(pl.col(\"name\") == c).count().alias(f\"{c}_name_counts{feature_suffix}\")for c in LIST_NAME_FEATURE],   \n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\")==c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in LIST_NAME_FEATURE],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\")==c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for c in LIST_NAME_FEATURE],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\")==c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in LIST_NAME_FEATURE],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\")==c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in LIST_NAME_FEATURE],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"name\")==c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in LIST_NAME_FEATURE],\n",
    "\n",
    "        *[pl.col(\"room_fqid\").filter(pl.col(\"room_fqid\") == c).count().alias(f\"{c}_room_fqid_counts{feature_suffix}\")for c in LIST_ROOM],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in LIST_ROOM],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in LIST_ROOM],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for c in LIST_ROOM],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in LIST_ROOM],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"room_fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in LIST_ROOM],\n",
    "\n",
    "        *[pl.col(\"fqid\").filter(pl.col(\"fqid\") == c).count().alias(f\"{c}_fqid_counts{feature_suffix}\")for c in LIST_FQID],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in LIST_FQID],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in LIST_FQID],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for c in LIST_FQID],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in LIST_FQID],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in LIST_FQID],\n",
    "\n",
    "        *[pl.col(\"text_fqid\").filter(pl.col(\"text_fqid\") == c).count().alias(f\"{c}_text_fqid_counts{feature_suffix}\") for c in LIST_TEXT],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in LIST_TEXT],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in LIST_TEXT],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for c in LIST_TEXT],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in LIST_TEXT],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"text_fqid\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in LIST_TEXT],\n",
    "\n",
    "        *[pl.col(\"location_x_diff\").filter(pl.col(\"event_name\") == c).mean().alias(f\"{c}_ET_mean_x{feature_suffix}\") for c in LIST_EVENT_NAME_FEATURE],\n",
    "        *[pl.col(\"location_x_diff\").filter(pl.col(\"event_name\") == c).median().alias(f\"{c}_ET_median_x{feature_suffix}\") for c in LIST_EVENT_NAME_FEATURE],\n",
    "        *[pl.col(\"location_x_diff\").filter(pl.col(\"event_name\") == c).std().alias(f\"{c}_ET_std_x{feature_suffix}\") for c in LIST_EVENT_NAME_FEATURE],\n",
    "        *[pl.col(\"location_x_diff\").filter(pl.col(\"event_name\") == c).max().alias(f\"{c}_ET_max_x{feature_suffix}\") for c in LIST_EVENT_NAME_FEATURE],\n",
    "        *[pl.col(\"location_x_diff\").filter(pl.col(\"event_name\") == c).min().alias(f\"{c}_ET_min_x{feature_suffix}\") for c in LIST_EVENT_NAME_FEATURE],\n",
    "\n",
    "        *[pl.col(\"level\").filter(pl.col(\"level\") == c).count().alias(f\"{c}_LEVEL_count{feature_suffix}\") for c in LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).std().alias(f\"{c}_ET_std_{feature_suffix}\") for c in LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).mean().alias(f\"{c}_ET_mean_{feature_suffix}\") for c in LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).sum().alias(f\"{c}_ET_sum_{feature_suffix}\") for c in LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).median().alias(f\"{c}_ET_median_{feature_suffix}\") for c in LEVELS],\n",
    "        *[pl.col(\"elapsed_time_diff\").filter(pl.col(\"level\") == c).max().alias(f\"{c}_ET_max_{feature_suffix}\") for c in LEVELS],\n",
    "        ]\n",
    "\n",
    "    df = x.groupby([\"session_id\"], maintain_order=True).agg(aggs).sort(\"session_id\")\n",
    "\n",
    "    dict_agg_feature_by_level = {\n",
    "        '0-4':[\n",
    "            pl.col(\"elapsed_time\").filter(((pl.col('fqid') == 'tunic') & (pl.col('event_name') == 'navigate_click')) | (pl.col('text_fqid') == 'tunic.historicalsociety.collection.tunic.slip')).apply(except_handling).alias(\"slip_click_duration\"),\n",
    "            pl.col(\"index\").filter(((pl.col('fqid') == 'tunic') & (pl.col('event_name') == 'navigate_click')) | (pl.col('text_fqid') == 'tunic.historicalsociety.collection.tunic.slip')).apply(except_handling).alias(\"slip_click_indexCount\"),\n",
    "            pl.col(\"elapsed_time\").filter(((pl.col('fqid') == 'plaque') & (pl.col('event_name') == 'navigate_click')) | (pl.col('text_fqid') == 'tunic.kohlcenter.halloffame.plaque.face.date')).apply(except_handling).alias(\"shirt_era_search_duration\"),\n",
    "            pl.col(\"index\").filter(((pl.col('fqid') == 'plaque') & (pl.col('event_name') == 'navigate_click')) | (pl.col('text_fqid') == 'tunic.kohlcenter.halloffame.plaque.face.date')).apply(except_handling).alias(\"shirt_era_search_indexCount\"),\n",
    "        ],\n",
    "        '5-12':[\n",
    "            pl.col(\"elapsed_time\").filter((pl.col(\"text\")==\"Here's the log book.\")|(pl.col(\"fqid\")=='logbook.page.bingo')).apply(lambda s: s.max()-s.min()).alias(\"logbook_bingo_duration\"),\n",
    "            pl.col(\"index\").filter((pl.col(\"text\")==\"Here's the log book.\")|(pl.col(\"fqid\")=='logbook.page.bingo')).apply(lambda s: s.max()-s.min()).alias(\"logbook_bingo_indexCount\"),\n",
    "            pl.col(\"elapsed_time\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='reader'))|(pl.col(\"fqid\")==\"reader.paper2.bingo\")).apply(lambda s: s.max()-s.min()).alias(\"reader_bingo_duration\"),\n",
    "            pl.col(\"index\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='reader'))|(pl.col(\"fqid\")==\"reader.paper2.bingo\")).apply(lambda s: s.max()-s.min()).alias(\"reader_bingo_indexCount\"),\n",
    "            pl.col(\"elapsed_time\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='journals'))|(pl.col(\"fqid\")==\"journals.pic_2.bingo\")).apply(lambda s: s.max()-s.min()).alias(\"journals_bingo_duration\"),\n",
    "            pl.col(\"index\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='journals'))|(pl.col(\"fqid\")==\"journals.pic_2.bingo\")).apply(lambda s: s.max()-s.min()).alias(\"journals_bingo_indexCount\"),\n",
    "            pl.col(\"elapsed_time\").filter(((pl.col('fqid') == 'businesscards') & (pl.col('event_name') == 'navigate_click')) | (pl.col('text_fqid') == 'tunic.humanecology.frontdesk.businesscards.card_bingo.bingo')).apply(except_handling).alias(\"businesscard_bingo_duration\"),\n",
    "            pl.col(\"index\").filter(((pl.col('fqid') == 'businesscards') & (pl.col('event_name') == 'navigate_click')) | (pl.col('text_fqid') == 'tunic.humanecology.frontdesk.businesscards.card_bingo.bingo')).apply(except_handling).alias(\"businesscard_bingo_indexCount\"),\n",
    "\n",
    "            # add 20230423\n",
    "            pl.col(\"elapsed_time\").filter(((pl.col('text_fqid') == \"tunic.historicalsociety.frontdesk.archivist.need_glass_0\")) | (pl.col('text_fqid') == \"tunic.historicalsociety.frontdesk.magnify\")).apply(except_handling).alias(\"search_grass_duration\"),\n",
    "            pl.col(\"index\").filter(((pl.col('text_fqid') == \"tunic.historicalsociety.frontdesk.archivist.need_glass_0\")) | (pl.col('text_fqid') == \"tunic.historicalsociety.frontdesk.magnify\")).apply(except_handling).alias(\"search_grass_indexCount\"),\n",
    "\n",
    "            # add 20230506\n",
    "            pl.col(\"elapsed_time\").filter(pl.col('level_group') == '0-4').last().alias('level_1_last'),\n",
    "            pl.col(\"elapsed_time\").filter(pl.col('level_group') == '5-12').first().alias('level_2_first'),\n",
    "\n",
    "            *[pl.col(c).filter(pl.col('level_group') == '5-12').mean().alias(f\"{c}_mean_level2\") for c in LIST_NUMS],\n",
    "            *[pl.col(c).filter(pl.col('level_group') == '5-12').median().alias(f\"{c}_median_level2\") for c in LIST_NUMS],\n",
    "            *[pl.col(c).filter(pl.col('level_group') == '5-12').std().alias(f\"{c}_std_level2\") for c in LIST_NUMS],\n",
    "            *[pl.col(c).filter(pl.col('level_group') == '5-12').min().alias(f\"{c}_min_level2\") for c in LIST_NUMS],\n",
    "            *[pl.col(c).filter(pl.col('level_group') == '5-12').max().alias(f\"{c}_max_level2\") for c in LIST_NUMS],\n",
    "            *[pl.col(c).filter(pl.col('level_group') == '5-12').sum().alias(f\"{c}_sum_level2\") for c in LIST_NUMS],\n",
    "\n",
    "        ],\n",
    "        '13-22':[\n",
    "            pl.col(\"elapsed_time\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='reader_flag'))|(pl.col(\"fqid\")==\"tunic.library.microfiche.reader_flag.paper2.bingo\")).apply(lambda s: s.max()-s.min() if s.len()>0 else 0).alias(\"reader_flag_duration\"),\n",
    "            pl.col(\"index\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='reader_flag'))|(pl.col(\"fqid\")==\"tunic.library.microfiche.reader_flag.paper2.bingo\")).apply(lambda s: s.max()-s.min() if s.len()>0 else 0).alias(\"reader_flag_indexCount\"),\n",
    "            pl.col(\"elapsed_time\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='journals_flag'))|(pl.col(\"fqid\")==\"journals_flag.pic_0.bingo\")).apply(lambda s: s.max()-s.min() if s.len()>0 else 0).alias(\"journalsFlag_bingo_duration\"),\n",
    "            pl.col(\"index\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='journals_flag'))|(pl.col(\"fqid\")==\"journals_flag.pic_0.bingo\")).apply(lambda s: s.max()-s.min() if s.len()>0 else 0).alias(\"journalsFlag_bingo_indexCount\"),\n",
    "            pl.col(\"elapsed_time\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='tracks'))|(pl.col(\"text\")==\"That hoofprint doesn't match the flag!\")).apply(except_handling).alias(\"tracks_duration\"),\n",
    "            pl.col(\"index\").filter(((pl.col(\"event_name\")=='navigate_click')&(pl.col(\"fqid\")=='tracks'))|(pl.col(\"text\")==\"That hoofprint doesn't match the flag!\")).apply(except_handling).alias(\"tracks_indexCount\"),\n",
    "\n",
    "            # add 20230423\n",
    "            pl.col(\"elapsed_time\").filter(((pl.col('event_name') == \"person_click\") & (pl.col('text_fqid') == \"tunic.historicalsociety.cage.teddy.trapped\")) | ((pl.col('event_name') == \"navigate_click\") & (pl.col('fqid') == \"unlockdoor\"))).apply(except_handling).alias(\"search_key_duration\"),\n",
    "            pl.col(\"index\").filter(((pl.col('event_name') == \"person_click\") & (pl.col('text_fqid') == \"tunic.historicalsociety.cage.teddy.trapped\")) | ((pl.col('event_name') == \"navigate_click\") & (pl.col('fqid') == \"unlockdoor\"))).apply(except_handling).alias(\"search_key_indexCount\"),\n",
    "\n",
    "            # add 20230506\n",
    "            pl.col(\"elapsed_time\").filter(pl.col('level_group') == '5-12').last().alias('level_2_last'),\n",
    "            pl.col(\"elapsed_time\").filter(pl.col('level_group') == '13-22').first().alias('level_3_first'),\n",
    "\n",
    "            *[pl.col(c).filter(pl.col('level_group') == '13-22').mean().alias(f\"{c}_mean_level3\") for c in LIST_NUMS],\n",
    "            *[pl.col(c).filter(pl.col('level_group') == '13-22').median().alias(f\"{c}_median_level3\") for c in LIST_NUMS],\n",
    "            *[pl.col(c).filter(pl.col('level_group') == '13-22').std().alias(f\"{c}_std_level3\") for c in LIST_NUMS],\n",
    "            *[pl.col(c).filter(pl.col('level_group') == '13-22').min().alias(f\"{c}_min_level3\") for c in LIST_NUMS],\n",
    "            *[pl.col(c).filter(pl.col('level_group') == '13-22').max().alias(f\"{c}_max_level3\") for c in LIST_NUMS],\n",
    "            *[pl.col(c).filter(pl.col('level_group') == '13-22').sum().alias(f\"{c}_sum_level3\") for c in LIST_NUMS],\n",
    "\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    if use_extra:\n",
    "        if grp == '0-4':\n",
    "            aggs = dict_agg_feature_by_level['0-4']\n",
    "            tmp = x.groupby([\"session_id\"], maintain_order=True).agg(aggs).sort(\"session_id\")\n",
    "            df = df.join(tmp, on=\"session_id\", how='left')\n",
    "\n",
    "        if grp == '5-12':\n",
    "            aggs = dict_agg_feature_by_level['0-4'] + dict_agg_feature_by_level['5-12']\n",
    "\n",
    "            tmp = (x.groupby([\"session_id\"], maintain_order=True)\n",
    "                    .agg(aggs)\n",
    "                    .sort(\"session_id\")\n",
    "                    .with_columns(level1_answer_time=pl.col('level_2_first') - pl.col('level_1_last'))\n",
    "                    .drop(['level_1_last', 'level_2_first']))\n",
    "\n",
    "            df = df.join(tmp, on=\"session_id\", how='left')\n",
    "\n",
    "        if grp == '13-22':\n",
    "            aggs = dict_agg_feature_by_level['0-4'] + dict_agg_feature_by_level['5-12'] + dict_agg_feature_by_level['13-22']\n",
    "\n",
    "            tmp = (x.groupby([\"session_id\"], maintain_order=True)\n",
    "                    .agg(aggs)\n",
    "                    .sort(\"session_id\")\n",
    "                    .with_columns(level1_answer_time=pl.col('level_2_first') - pl.col('level_1_last'))\n",
    "                    .with_columns(level2_answer_time=pl.col('level_3_first') - pl.col('level_2_last'))\n",
    "                    .drop(['level_1_last', 'level_2_first', 'level_2_last', 'level_3_first']))\n",
    "\n",
    "            df = df.join(tmp, on=\"session_id\", how='left')\n",
    "\n",
    "    return df.to_pandas()\n",
    "\n",
    "\n",
    "def time_feature(df):\n",
    "    df[\"year\"] = df[\"session_id\"].apply(lambda x: int(str(x)[:2])).astype(np.uint8)\n",
    "    df[\"month\"] = df[\"session_id\"].apply(lambda x: int(str(x)[2:4])+1).astype(np.uint8)\n",
    "    df[\"day\"] = df[\"session_id\"].apply(lambda x: int(str(x)[4:6])).astype(np.uint8)\n",
    "    df[\"hour\"] = df[\"session_id\"].apply(lambda x: int(str(x)[6:8])).astype(np.uint8)\n",
    "    df[\"minute\"] = df[\"session_id\"].apply(lambda x: int(str(x)[8:10])).astype(np.uint8)\n",
    "    df[\"second\"] = df[\"session_id\"].apply(lambda x: int(str(x)[10:12])).astype(np.uint8)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T12:14:10.856460Z",
     "iopub.status.busy": "2023-06-13T12:14:10.855852Z",
     "iopub.status.idle": "2023-06-13T12:14:10.863429Z",
     "shell.execute_reply": "2023-06-13T12:14:10.862159Z",
     "shell.execute_reply.started": "2023-06-13T12:14:10.856388Z"
    }
   },
   "outputs": [],
   "source": [
    "import jo_wilder_310 as jo_wilder\n",
    "\n",
    "try:\n",
    "    jo_wilder.make_env.__called__ = False\n",
    "    env.__called__ = False\n",
    "    type(env)._state = type(type(env)._state).__dict__['INIT']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "env = jo_wilder.make_env()\n",
    "iter_test = env.iter_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-13T12:14:11.777067Z",
     "iopub.status.busy": "2023-06-13T12:14:11.775854Z",
     "iopub.status.idle": "2023-06-13T12:14:26.758619Z",
     "shell.execute_reply": "2023-06-13T12:14:26.757499Z",
     "shell.execute_reply.started": "2023-06-13T12:14:11.777031Z"
    }
   },
   "outputs": [],
   "source": [
    "limits = {'0-4':(1,4), '5-12':(4,14), '13-22':(14,19)}\n",
    "list_q = {'0-4':quests_0_4, '5-12':quests_5_12, '13-22':quests_13_22}\n",
    "dict_feature_suffix = {'0-4': 'grp1', '5-12': 'grp2', '13-22': 'grp3'}\n",
    "\n",
    "dict_feature_pre_raw = {}\n",
    "session_id = -9999\n",
    "\n",
    "for (test, sample_submission) in iter_test:\n",
    "    test = test.sort_values(by='index')  # NOTE: jo_wilder_310 API\n",
    "    # sort question\n",
    "    sample_submission['questions'] = sample_submission['session_id'].str.split('_q').apply(lambda x: int(x[1]))\n",
    "    sample_submission = sample_submission.sort_values(by='questions', ascending=True).reset_index()\n",
    "    sample_submission.loc[sample_submission.questions.isin([5, 8, 10, 13, 15]), 'correct'] = 0  \n",
    "    sample_submission = sample_submission[['session_id', 'correct']]\n",
    "\n",
    "    if session_id == test.session_id.values[0]:\n",
    "        is_switch_id = True\n",
    "    else:\n",
    "        is_switch_id = False\n",
    "\n",
    "    session_id = test.session_id.values[0]\n",
    "    grp = test.level_group.values[0]\n",
    "    a, b = limits[grp]\n",
    "    old_train = delt_time_def(test[test.level_group == grp])\n",
    "\n",
    "    # stacking用のデータフレームを初期化しておく\n",
    "    if not is_switch_id:\n",
    "        df_pred_pre_raw = pd.DataFrame({'session_id': [session_id]})\n",
    "\n",
    "    feature_pre_raw = DICT_FEATURE_LIST['pre_raw'][grp].copy()\n",
    "\n",
    "    # pre_rawモデル用のデータフレーム\n",
    "    test = pl.from_pandas(test)\n",
    "    test = preprocessing_pl(test)\n",
    "    dict_feature_pre_raw[grp] = test\n",
    "\n",
    "    # 特徴量作成(pre_raw)\n",
    "    if grp == '5-12':\n",
    "        test = pl.concat([dict_feature_pre_raw['0-4'], test])\n",
    "    elif grp == '13-22':\n",
    "        test = pl.concat([dict_feature_pre_raw['0-4'], dict_feature_pre_raw['5-12'], test])\n",
    "\n",
    "    test = feature_engineer_pl(test, grp, use_extra=IS_USE_EXTRA, feature_suffix='')\n",
    "    test = time_feature(test)\n",
    "    test = test[['session_id'] + feature_pre_raw]\n",
    "\n",
    "    for t in range(a, b):\n",
    "        mask = sample_submission.session_id.str.endswith(f'q{t}')\n",
    "        # CatBoost Part\n",
    "        if t in list_q[grp]:\n",
    "            new_train = feature_engineer(old_train, list_kol_f[t])\n",
    "            new_train = feature_quest_otvet(new_train, old_train, t, list_kol_f[t])\n",
    "\n",
    "            clf_cat = models[f'{t}']\n",
    "            pred_cat = clf_cat.predict_proba(new_train.astype('float32'))[:,1]\n",
    "        else:\n",
    "            pred_cat = np.array([1])\n",
    "\n",
    "        # XGBoost Part\n",
    "        clf_xgb = XGBClassifier()\n",
    "        clf_xgb.load_model(PATH_MODEL_PRE_RAW.format(t))\n",
    "\n",
    "        if t == 1:\n",
    "            df_feature_pre_raw = test.drop(columns=['session_id'])\n",
    "        elif t > 1:\n",
    "            df_feature_pre_raw = pd.merge(test, df_pred_pre_raw.iloc[:, :t], on='session_id', how='left').drop(columns=['session_id'])\n",
    "\n",
    "        pred_xgb = clf_xgb.predict_proba(df_feature_pre_raw.astype('float32'))[:,1]\n",
    "        df_pred_pre_raw[f'q_{t}'] = pred_xgb\n",
    "\n",
    "        # blend\n",
    "        p = (pred_cat * 0.3) + (pred_xgb * 0.7)\n",
    "\n",
    "        sample_submission.loc[mask,'correct'] = np.where(p > threshold, int(1), int(0))\n",
    "\n",
    "    env.predict(sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
